# This script processes the raw point clouds from the CMU Panoptic dataset [1],
# sequence '171026_pose3' which contains only one human subject. The original
# sequence contains multiple chunks, where the human is static, therefore it
# is split into 4 subsequences used in the paper (the static chunks are
# removed). The script clusters the subject, removes the outlier points and
# samples 10k pts for each final sample. It saves the data for all the
# sequences in one common file. It saves the following:
#
# - sampled points
# - cummulative index ranges for the individual sequences
#
# [1] H. Joo et al. The Panoptic Studio: A Massively Multiview System for
# Social Motion Capture. ICCV 2015.
#

# 3rd party.
import numpy as np
import trimesh
import open3d as o3d
from scipy.spatial.transform import Rotation
import yaml

# Project files.
import jblib.file_sys as jbfs

# Python std.
from timeit import default_timer as timer

################################################################################
# Settings.
################################################################################

# Paths.
path_pclouds = ''  # <-- Path to the point clouds for the sequence '171026_pose3' generated by the CMU toolkit.
path_ds_out = ''  # <-- Path to the root dir for the generated data.
name_pts = 'pts_10k.npy'
name_tf = 'tf.taml'

# Final pcloud.
num_pts = 10000

# Pcloud thresholds.
th_y_floor = 5.0

# Clustering.
eps = 0.1
min_pts = 250

# Outlier removal.
nn = 20
stdr = 1.25

# Processing.
save_every = 200

# Subsequences frame range inds.
fr_pose_1 = [342, 752]
fr_pose_2 = [1387, 3177]
fr_pose_3 = [4412, 5297]
fr_pose_4 = [6727, 7277]

################################################################################
# Helpers.
################################################################################

# Constants.
R_std_coords = Rotation.from_euler('xyz', [np.pi, 0., 0.])


def store(x, sc, offs, path_pts, path_tf):
    np.save(path_pts, x)
    with open(path_tf, 'w') as f:
        yaml.dump({'scale': float(sc), 'offset': offs.tolist()}, f)


################################################################################
# Main script.
################################################################################

# Seq to inds mapping.
seq_lens = [r[1] - r[0] for r in [fr_pose_1, fr_pose_2, fr_pose_3, fr_pose_4]]
seq_rangs = np.concatenate(
    [np.zeros((1, ), dtype=np.int64), np.cumsum(seq_lens)], axis=0)
seq2ind = {
    '171026_pose3_waving_arms': [seq_rangs[0], seq_rangs[1]],
    '171026_pose3_low_lunges': [seq_rangs[1], seq_rangs[2]],
    '171026_pose3_crossing_arms': [seq_rangs[2], seq_rangs[3]],
    '171026_pose3_arms_fingers': [seq_rangs[3], seq_rangs[4]]
}

# Output path.
path_out_dir = jbfs.jn(path_ds_out, '171026_pose3')
jbfs.make_dir(path_out_dir)
path_out_pts = jbfs.jn(path_out_dir, name_pts)
path_out_tf = jbfs.jn(path_out_dir, name_tf)

# Load all files.
files = jbfs.ls(path_pclouds, exts='ply')

# Process all files.
sc, offs = None, None
pcs_all = []
ts = timer()
tela, trem, sps = 0., 0., 0.
finds = np.concatenate([np.arange(
    r[0], r[1]) for r in [fr_pose_1, fr_pose_2, fr_pose_3, fr_pose_4]], axis=0)
scale_saved = False
for fi in finds:
    print(f"\rProcessing sample {fi + 1}/{finds.shape[0]}, "
          f"elpsd: {tela / 60.:.2f} min, rem: {trem / 60.:.2f} min. "
          f" ({sps:.2f} smpls/s.)", end='')

    # Load pcloud.
    x = np.asarray(trimesh.load_mesh(jbfs.jn(
        path_pclouds, files[fi]), process=False).vertices)

    # Transform to the standardized coord. sys (x-right, y-up, -z-forward).
    x = R_std_coords.apply(x)

    # Remove the floor points.
    x = x[x[:, 1] > th_y_floor]

    # Scale down.
    if not scale_saved:
        sc = np.max(np.max(x[:, 1], axis=0) - np.min(x[:, 1], axis=0))
        scale_saved = True
    x = x / sc

    # Create an o3d pcloud.
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(x.astype(np.float32))

    # Subsample.
    subsample = max(1, round(x.shape[0] / 25000))
    pcd = pcd.uniform_down_sample(every_k_points=subsample)

    # Cluster (keep only the biggest cluster).
    labels = np.array(pcd.cluster_dbscan(
        eps=eps, min_points=min_pts))
    num_clusts = np.max(labels) + 1
    ind_max_cl = 0
    if num_clusts == 0:
        print(f"[ERROR]: 0 clusters found.")
    elif num_clusts > 1:
        ind_max_cl = np.argmax(np.sum(labels[:, None] == np.arange(
            num_clusts)[None], axis=0))
    pcd = pcd.select_by_index(np.where(labels == ind_max_cl)[0])

    # Remove outlier points.
    pcd = pcd.remove_statistical_outlier(
        nb_neighbors=nn, std_ratio=stdr)[0]

    # Convert to np and sample 10k pts.
    pc = np.asarray(pcd.points).astype(np.float32)
    if pc.shape[0] < num_pts:
        print(f"[WARNING]: The resulting pcloud has less than "
              f"{num_pts} pts ({pc.shape[0]}), duplicating some "
              f"points.")
    rinds = np.random.permutation(pc.shape[0])[:num_pts]
    while rinds.shape[0] != num_pts:
        rinds = np.concatenate(
            [rinds, np.random.permutation(pc.shape[0])],
            axis=0)[:num_pts]
    pc = pc[rinds]

    # Move the pcloud to the center along the ground (y) plane.
    if fi == 0:
        offs = np.mean(pc, axis=0)
        offs[1] = 0.
    pc = pc - offs

    # Save.
    pcs_all.append(pc)

    # Time stats.
    tela = timer() - ts
    sps = (fi + 1) / tela
    trem = (len(files) - fi - 1) / sps

    # Store to disk.
    if (fi + 1) % save_every == 0:
        store(np.stack(pcs_all, axis=0), sc, offs, path_out_pts, path_out_tf)

# Store to disk.
store(np.stack(pcs_all, axis=0), sc, offs, path_out_pts, path_out_tf)
